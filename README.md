# llama_gguf_local_gpt

# О чем?
Позволяет запустить локальную gguf модель lama на своем сервере Flask. Можно как задавать вопросы модели, так и осуществлять разговор по конкретным источникам данных(локальных)

# Что нужно для работы?
Python: 3.8 -3.11,
llama-cpp(https://github.com/abetlen/llama-cpp-python),
Flask,
tiktoken,
numpy,
faiss,
markdown,
PyMuPDF,
mammoth,
docx

# Принцип работы
При помощи faiss_index.py создаем индексированную разметку наших локальных источников данных.
При помощи Llama_flask.py запускаем сервер flask.

Обязательно укажите все пути и настройки для моделей в файлах .py.

## В файле Llama_flask.py
В данном файле производится эмбеддинг вопроса и осуществляется поиск ближайшего куска текста из источника. Затем он передается в модель вместе с вопросом. На выходе получается ответ именно модели llama и указывается источник, где был найден ответ.

Так же можно задать вопрос напрямую модели llama, предварительно сняв галочку с "Использовать источники"
